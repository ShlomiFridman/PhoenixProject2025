{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "github link: https://github.com/ShlomiFridman/PhoenixProject2025"
      ],
      "metadata": {
        "id": "8GkvwvPQRDnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wzvWI3tGhOqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkC_PXGpO64W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289a514a-6d13-46ce-92a8-ff5debd8bf25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: firebase in /usr/local/lib/python3.10/dist-packages (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firebase) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firebase) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "!pip install requests beautifulsoup4 nltk\n",
        "!pip install firebase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.robotparser import RobotFileParser\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from firebase import firebase\n",
        "# import spacy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Markdown, SVG\n",
        "from collections import defaultdict\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from nltk.chat.util import Chat, reflections\n",
        "import difflib\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Q-Dcy13DQEaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils functions"
      ],
      "metadata": {
        "id": "67nX-B8nV2MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def read_txtfile(fileName):\n",
        "#     file_path = '/content/drive/My Drive/' + fileName\n",
        "#     with open(file_path, 'r') as file:\n",
        "#         allText = \"\"\n",
        "#         for line in file:\n",
        "#           allText += line\n",
        "#         wordsList =  allText.split()\n",
        "#         wordsSet =  set(wordsList)\n",
        "#     return wordsSet\n",
        "\n",
        "def index_words(soup):\n",
        "    index_res = {}\n",
        "    words = re.findall(r'\\w+', soup.get_text())\n",
        "\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "\n",
        "        if word in index_res:\n",
        "            index_res[word] += 1\n",
        "        else:\n",
        "            index_res[word] = 1\n",
        "\n",
        "    return index_res\n",
        "\n",
        "def remove_stop_words(p_index):\n",
        "    # nlp = spacy.load(\"en_core_web_sm\")  # Load a small English model\n",
        "    # stop_words = nlp.Defaults.stop_words\n",
        "    stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to'}\n",
        "    # stop_words = read_txtfile(\"stopwords_en.txt\")\n",
        "\n",
        "    for stop_word in stop_words:\n",
        "        if stop_word in p_index:\n",
        "            del p_index[stop_word]\n",
        "\n",
        "    return p_index\n",
        "\n",
        "def apply_stemming(p_index):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_index = {}\n",
        "\n",
        "    for word, count in p_index.items():\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "\n",
        "        if stemmed_word in stemmed_index:\n",
        "            stemmed_index[stemmed_word] += count\n",
        "        else:\n",
        "            stemmed_index[stemmed_word] = count\n",
        "\n",
        "    return stemmed_index"
      ],
      "metadata": {
        "id": "UE9GtuIyV11Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firebase service"
      ],
      "metadata": {
        "id": "dE6sX52BRZ5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirebaseService:\n",
        "\n",
        "  def __init__(self, db_url = 'https://phoenixhw2-default-rtdb.europe-west1.firebasedatabase.app/'):\n",
        "    self.FBconn = firebase.FirebaseApplication(db_url,None)\n",
        "\n",
        "  def get_rev_index_from_DB(self):\n",
        "    return self.FBconn.get('/Index/', None)\n",
        "\n",
        "  def update_rev_index_in_db(self, index_p):\n",
        "    self.FBconn.delete(\"/Index/\", None)\n",
        "    print(\"Cleared database before save\")\n",
        "    for k,v in index_p.items():\n",
        "      self.FBconn.put(\"/Index/\", k, v)\n",
        "      print(f\"update made for index={k} ({v['term']})\")\n",
        "    print(\"Updated given rev index in DB\")\n",
        "\n",
        "  # def get_url_index_from_DB(self):\n",
        "  #   return self.FBconn.get('/URLs_Index/', None)\n",
        "\n",
        "  # def update_url_index_in_db(self, index_p):\n",
        "  #   for k,v in index_p.items():\n",
        "  #     self.FBconn.put(\"/URLs_Index/\", k, v)\n",
        "  #     print(f\"update made for index_url={k}\")\n",
        "  #   print(\"Updated given url index in DB\")"
      ],
      "metadata": {
        "id": "iQFrRnWZbPqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "index service for maintain"
      ],
      "metadata": {
        "id": "A-PC6QK9RHqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IndexService:\n",
        "\n",
        "  def __init__(self, index = None, firebaseService = None):\n",
        "    self.default_index_words = [\n",
        "        'SAAS',\n",
        "        'PAAS',\n",
        "        'IAAS',\n",
        "        'FAAS',\n",
        "        'Private',\n",
        "        'Public',\n",
        "        'Hybrid',\n",
        "        'Service',\n",
        "        'Platform',\n",
        "        'Infrastructure',\n",
        "        'Study',\n",
        "        'Case',\n",
        "        'Chatbot',\n",
        "        'Engine',\n",
        "        'Cloud',\n",
        "        'Monitor',\n",
        "        'Data',\n",
        "        'Mainframe',\n",
        "        'Performance',\n",
        "        'Security',\n",
        "        'SLA',\n",
        "        'KPI',\n",
        "        'SOA',\n",
        "        'Information',\n",
        "        'Kafka',\n",
        "        'SQL',\n",
        "        'Technology',\n",
        "        'Database',\n",
        "        'Docker',\n",
        "        'Kubernetes',\n",
        "        'RabbitMQ',\n",
        "        'IBM',\n",
        "        'Google',\n",
        "        'Amazon',\n",
        "        'AI',\n",
        "        'Artificial',\n",
        "        'Intelligence',\n",
        "    ]\n",
        "    self.init_index_vals = index if index else self.default_index_words\n",
        "    self.stemmer = PorterStemmer()\n",
        "    self.firebaseService = firebaseService\n",
        "    self.resetService()\n",
        "\n",
        "  def process_soup(self, url, soup):\n",
        "    if not soup:\n",
        "        print(\"empty soup\")\n",
        "        return\n",
        "\n",
        "    u_index = index_words(soup)\n",
        "    u_index = remove_stop_words(u_index)\n",
        "    u_index = apply_stemming(u_index)\n",
        "    self.urls_index[url] = u_index\n",
        "\n",
        "    for ind, ind_val in self.rev_index.items():\n",
        "        if ind not in u_index:\n",
        "            continue\n",
        "        elif url in ind_val['DocIDs']:\n",
        "            url_i = ind_val[\"DocIDs\"].index(url)\n",
        "            del ind_val[\"DocIDs\"][url_i]\n",
        "            del ind_val[\"DocIDs_cntrs\"][url_i]\n",
        "\n",
        "            # Ensure 'DocIDs_snippet' exists before attempting to delete or append\n",
        "            if 'DocIDs_snippet' in ind_val and len(ind_val['DocIDs_snippet']) > url_i:\n",
        "                del ind_val[\"DocIDs_snippet\"][url_i]\n",
        "\n",
        "        # Initialize lists if they don't exist\n",
        "        if \"DocIDs\" not in ind_val:\n",
        "            ind_val['DocIDs'] = []\n",
        "        if \"DocIDs_cntrs\" not in ind_val:\n",
        "            ind_val['DocIDs_cntrs'] = []\n",
        "        if \"DocIDs_snippet\" not in ind_val:\n",
        "            ind_val['DocIDs_snippet'] = []  # Ensure it's always initialized\n",
        "\n",
        "        # Append to the lists\n",
        "        ind_val['DocIDs'].append(url)\n",
        "        ind_val['DocIDs_cntrs'].append(u_index[ind])\n",
        "\n",
        "        # Create the snippet (first 100 words)\n",
        "        snippet = \" \".join(soup.get_text().split()[:100])  # Ensure you get the first 100 words from the text\n",
        "        ind_val['DocIDs_snippet'].append(snippet)\n",
        "    return u_index\n",
        "\n",
        "  def get_reverse_index(self):\n",
        "    return self.rev_index\n",
        "\n",
        "  def get_url_index(self, url):\n",
        "    return self.urls_index.get(url,{})\n",
        "\n",
        "  def set_rev_index(self, newRevIndex):\n",
        "    self.init_index_vals = [vals['term'] for ind,vals in newRevIndex.items()]\n",
        "    self.rev_index = newRevIndex\n",
        "    self.urls_index = {}\n",
        "\n",
        "    for ind, vals in self.rev_index.items():\n",
        "      urls = vals['DocIDs'] if 'DocIDs' in vals else []\n",
        "      cntrs = vals['DocIDs_cntrs'] if 'DocIDs_cntrs' in vals else []\n",
        "      for j in range(len(vals['DocIDs'])):\n",
        "        if urls[j] not in self.urls_index:\n",
        "          self.urls_index[urls[j]] = {}\n",
        "        self.urls_index[urls[j]][ind] = cntrs[j]\n",
        "    # print(\"index updated\")\n",
        "\n",
        "  def get_index_of_word(self, w):\n",
        "    stemmed_w = self.stemmer.stem(w)\n",
        "    return self.rev_index.get(stemmed_w, None)\n",
        "\n",
        "  def add_new_word(self, word):\n",
        "    stemmedWord = self.stemmer.stem(word)\n",
        "    if (stemmedWord in self.rev_index):\n",
        "      return False\n",
        "    self.init_index_vals.append(word)\n",
        "    self.rev_index[stemmedWord] = {\"term\": word, \"DocIDs\": [], \"DocIDs_cntrs\": []}\n",
        "    return True\n",
        "\n",
        "  def remove_word(self, word):\n",
        "    stemmedWord = self.stemmer.stem(word)\n",
        "    if stemmedWord not in self.rev_index:\n",
        "      return False\n",
        "    self.init_index_vals.remove(self.rev_index[stemmedWord]['term'])\n",
        "    del self.rev_index[stemmedWord]\n",
        "    return True\n",
        "\n",
        "  def remove_url(self, url):\n",
        "    if url not in self.urls_index:\n",
        "      return False\n",
        "    for word,cntr in self.urls_index[url].items():\n",
        "      if word not in self.rev_index:\n",
        "        continue\n",
        "      ind = self.rev_index[word][\"DocIDs\"].index(url)\n",
        "      del self.rev_index[word][\"DocIDs\"][ind]\n",
        "      del self.rev_index[word][\"DocIDs_cntrs\"][ind]\n",
        "    del self.urls_index[url]\n",
        "    return True\n",
        "\n",
        "  def save_in_db(self):\n",
        "    if self.firebaseService:\n",
        "      self.firebaseService.update_rev_index_in_db(self.rev_index)\n",
        "    else:\n",
        "      print(\"IndexService does not have FirebaseService\")\n",
        "  def load_from_db(self):\n",
        "    if self.firebaseService:\n",
        "      db_index = self.firebaseService.get_rev_index_from_DB()\n",
        "      if db_index:\n",
        "        self.set_rev_index(db_index)\n",
        "    else:\n",
        "      print(\"IndexService does not have FirebaseService\")\n",
        "\n",
        "  def index_toString(self, minView=True):\n",
        "    str = ''\n",
        "    for ind,vals in self.rev_index.items():\n",
        "      str += f\"Index '{ind}'\\n\"\n",
        "      str += f\"\\tTerm='{vals['term']}'\\n\"\n",
        "      str += \"\\tDocIDs=\\n\"\n",
        "      if not minView:\n",
        "        for j in range(len(vals['DocIDs'])):\n",
        "          str += f\"\\t\\tURL No.{j}: {vals['DocIDs'][j]} - {vals['DocIDs_cntrs'][j]} times\\n\"\n",
        "      else:\n",
        "        str += f\"\\t\\tTotal Urls: {len(vals['DocIDs'])}\\n\"\n",
        "        str += f\"\\t\\tTotal Occurrences: {sum(vals['DocIDs_cntrs'])}\\n\"\n",
        "    return str\n",
        "\n",
        "  def resetService(self):\n",
        "    self.rev_index = {}\n",
        "    self.urls_index = {}\n",
        "    for w in self.init_index_vals:\n",
        "      self.rev_index[self.stemmer.stem(w)] = {\"term\":w, \"DocIDs\": [], \"DocIDs_cntrs\": []}\n"
      ],
      "metadata": {
        "id": "MFdi6YSbRVtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawling service"
      ],
      "metadata": {
        "id": "xOZCfKrsRQyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrawlerService:\n",
        "\n",
        "  def __init__(self, indexService, baseURLs=set(['https://www.ibm.com/us-en', 'https://www.ibm.com/topics']) ,maxDepth=100):\n",
        "    self.indexService = indexService\n",
        "    self.maxDepth = maxDepth\n",
        "    self.resetService()\n",
        "    self.baseURLs = baseURLs\n",
        "\n",
        "  def initCrawlingProcess(self, max_depth=10):\n",
        "    self.resetService()\n",
        "    for url in self.baseURLs:\n",
        "      self.crawl_website(url, max_depth)\n",
        "\n",
        "    # Function to crawl a website and fetch n pages\n",
        "  def crawl_website(self, base_url, max_pages=100):\n",
        "      rp = self.__check_robot(base_url)  # Check the robots.txt file\n",
        "      urls_to_crawl = [base_url]  # Initialize the queue with the base URL\n",
        "      current_crawled_urls = set()\n",
        "      ignore_urls = [\"form\", \"mp3\", \"mp4\", \"downloads\", \"zip\"]\n",
        "\n",
        "      while urls_to_crawl and len(current_crawled_urls) < max_pages:\n",
        "          current_url = urls_to_crawl.pop(0)\n",
        "\n",
        "          if current_url in self.crawled_urls:\n",
        "              continue  # Skip already crawled URLs\n",
        "\n",
        "          print(f\"Crawling {self.crawled_count+1}: {current_url}\")\n",
        "          page_content = self.__fetch_page_crawler(current_url, rp)\n",
        "\n",
        "          if page_content:\n",
        "              self.crawled_urls.add(current_url)\n",
        "              current_crawled_urls.add(current_url)\n",
        "              self.crawled_count += 1\n",
        "\n",
        "              # Extract and queue new links to crawl\n",
        "              new_links = self.__extract_links(page_content, base_url)\n",
        "              for link in new_links:\n",
        "                  cond1 = any(bad_url in link for bad_url in ignore_urls)\n",
        "                  cond2 = any(u for u in self.crawled_urls if (f\"{u}#\" in link))\n",
        "                  cond = cond1 or cond2\n",
        "                  if link not in self.crawled_urls and link not in urls_to_crawl and not cond:\n",
        "                      urls_to_crawl.append(link)\n",
        "\n",
        "              soup = BeautifulSoup(page_content, 'html.parser')\n",
        "              self.indexService.process_soup(current_url, soup)\n",
        "              # Delay between requests to avoid overwhelming the server\n",
        "              time.sleep(2)  # Sleep for 2 seconds between requests (politeness)\n",
        "\n",
        "      print(f\"\\nCrawled {len(current_crawled_urls)} pages.\")\n",
        "      return current_crawled_urls\n",
        "\n",
        "  def crawl_single_url(self, url):\n",
        "      if url in self.crawled_urls:\n",
        "        return \"Url was already crawled\"\n",
        "      self.baseURLs.add(url)\n",
        "      rp = self.__check_robot(url)  # Check the robots.txt file\n",
        "      print(f\"Crawling {self.crawled_count+1}: {url}\")\n",
        "      page_content = self.__fetch_page_crawler(url, rp)\n",
        "\n",
        "      if page_content:\n",
        "        self.crawled_urls.add(url)\n",
        "        self.crawled_count += 1\n",
        "\n",
        "        soup = BeautifulSoup(page_content, 'html.parser')\n",
        "        self.indexService.process_soup(url, soup)\n",
        "        time.sleep(2)  # Sleep for 2 seconds between requests (politeness)\n",
        "        return \"Url was crawled successfully\"\n",
        "      else:\n",
        "        return \"Cannot crawl given url\"\n",
        "\n",
        "\n",
        "\n",
        "  def get_crawled_urls(self):\n",
        "    return self.crawled_urls\n",
        "\n",
        "  def resetService(self):\n",
        "    self.crawled_urls = set()\n",
        "    self.crawled_count = 0\n",
        "    self.robot = None\n",
        "\n",
        "  # Function to fetch and parse the robots.txt file to check permissions\n",
        "  def __check_robot(self, url):\n",
        "      robot_url = urljoin(url, '/robots.txt')\n",
        "      rp = RobotFileParser()\n",
        "      rp.set_url(robot_url)\n",
        "      # Fetch and parse robots.txt file\n",
        "      rp.read()\n",
        "      if rp:\n",
        "        self.robot = rp\n",
        "      return rp if rp else self.robot\n",
        "\n",
        "  # Function to fetch and parse a page\n",
        "  def __fetch_page_crawler(self, url, rp):\n",
        "      # Check if the URL is allowed to be crawled according to robots.txt\n",
        "      if not rp.can_fetch('*', url):  # '*' means all user agents\n",
        "          print(f\"Blocked by robots.txt: {url}\")\n",
        "          return None\n",
        "\n",
        "      try:\n",
        "          response = requests.get(url, timeout=5)\n",
        "          response.raise_for_status()  # Will raise an exception for 4xx or 5xx responses\n",
        "          return response.text\n",
        "      except requests.exceptions.RequestException as e:\n",
        "          print(f\"Error fetching {url}: {e}\")\n",
        "          return None\n",
        "\n",
        "  # Function to extract internal links from a page\n",
        "  def __extract_links(self, page_content, base_url):\n",
        "      soup = BeautifulSoup(page_content, 'html.parser')\n",
        "      links = set()\n",
        "\n",
        "      # Find all anchor tags and extract the href attribute\n",
        "      # TODO need to add all links that are in tags with property cta-type=\"local\n",
        "      for anchor in soup.find_all('a', href=True):\n",
        "          href = anchor['href']\n",
        "\n",
        "          # Resolve relative URLs to absolute URLs\n",
        "          full_url = urljoin(base_url, href)\n",
        "\n",
        "          # Only add links that are within the same domain (ibm.com)\n",
        "          if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
        "              links.add(full_url)\n",
        "      for tag in soup.find_all(attrs={'cta-type': 'local'}, href=True):\n",
        "        href = tag['href']\n",
        "        # Resolve relative URLs to absolute URLs\n",
        "        full_url = urljoin(base_url, href)\n",
        "\n",
        "        # Only add links that are within the same domain\n",
        "        if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
        "            links.add(full_url)\n",
        "\n",
        "      return links"
      ],
      "metadata": {
        "id": "40IJ0awQS-JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query Service"
      ],
      "metadata": {
        "id": "4g5nvEsjJHW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryService:\n",
        "\n",
        "  # enable boolean search\n",
        "\n",
        "  def __init__(self, indexService):\n",
        "    self.indexService = indexService\n",
        "    # self.query_history_results = {}   # query => urls\n",
        "    self.query_history = []   # query => urls\n",
        "\n",
        "  def query(self, query):\n",
        "    res=self.__query_process(query)\n",
        "    self.query_history.insert(0, {'query': query, 'results': res})\n",
        "    return res\n",
        "\n",
        "  def get_history(self):\n",
        "    return self.query_history\n",
        "\n",
        "  def __query_process(self, query):\n",
        "    if \"AND\" in query:\n",
        "      ind=query.find(\"AND\")\n",
        "      return self.__andResults(self.__query_process(query[:ind]), self.__query_process(query[ind+3:]))\n",
        "    elif \"OR\" in query:\n",
        "      ind=query.find(\"OR\")\n",
        "      return self.__orResults(self.__query_process(query[:ind]), self.__query_process(query[ind+2:]))\n",
        "    return self.__queryByString(query)\n",
        "\n",
        "  def __queryByString(self, query):\n",
        "    url_res_set = set()\n",
        "    query_words = set(re.findall(r'\\w+', query.lower()))\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_query = set()\n",
        "    rev_index = self.indexService.get_reverse_index()  # Use the correct reverse index here\n",
        "\n",
        "    for word in query_words:\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        stemmed_query.add(stemmed_word)\n",
        "        # Add URLs to the result set if stemmed_word exists in reverse index\n",
        "        if stemmed_word in rev_index:\n",
        "            url_res_set.update(rev_index[stemmed_word][\"DocIDs\"])\n",
        "\n",
        "    ranked_url_res = []\n",
        "    for url_val in url_res_set:\n",
        "        # Initialize snippet to a default value in case no snippet is found\n",
        "        snippet = \"No snippet available\"\n",
        "\n",
        "        # Retrieve the snippet for the current URL from the reverse index\n",
        "        for ind, ind_val in rev_index.items():  # Use the correct reverse index here\n",
        "            if url_val in ind_val['DocIDs']:\n",
        "                url_i = ind_val['DocIDs'].index(url_val)\n",
        "\n",
        "                # Ensure 'DocIDs_snippet' exists and has the snippet for the correct URL\n",
        "                if 'DocIDs_snippet' in ind_val and len(ind_val['DocIDs_snippet']) > url_i:\n",
        "                    snippet = ind_val['DocIDs_snippet'][url_i]\n",
        "                break  # Exit the loop once the snippet is found\n",
        "\n",
        "        # Append the result with the URL, rank, and snippet\n",
        "        ranked_url_res.append({\n",
        "            'url': url_val,\n",
        "            'rank': self.rank_url(url_val, stemmed_query),\n",
        "            'snippet': snippet\n",
        "        })\n",
        "\n",
        "    # Sort the result by rank in descending order\n",
        "    ranked_url_res = sorted(ranked_url_res, key=lambda item: item['rank'], reverse=True)\n",
        "    # add result to history\n",
        "    # self.query_history[query] = ranked_url_res\n",
        "    # self.query_history.append({'query':query, 'results':ranked_url_res})\n",
        "    # print(type(ranked_url_res))\n",
        "    return ranked_url_res\n",
        "\n",
        "  def __orResults(self, lst1, lst2):\n",
        "    combined_dict = {}\n",
        "\n",
        "    for item in lst1:\n",
        "        url = item['url']\n",
        "        rank = item['rank']\n",
        "        combined_dict[url] = max(combined_dict.get(url, float('-inf')), rank)\n",
        "    for item in lst2:\n",
        "        url = item['url']\n",
        "        rank = item['rank']\n",
        "        combined_dict[url] = max(combined_dict.get(url, float('-inf')), rank)\n",
        "\n",
        "    return sorted(\n",
        "        [{'url': url, 'rank': rank} for url, rank in combined_dict.items()],\n",
        "        key=lambda item: item['rank'],\n",
        "        reverse=True)\n",
        "\n",
        "  def rank_url(self, url, query_words):\n",
        "    rank = 1\n",
        "    # resultService = ResultService()\n",
        "    # rank based on lab6\n",
        "    url_index = self.indexService.get_url_index(url)\n",
        "    for word in query_words:\n",
        "      if word in url_index:\n",
        "        rank = rank*1/url_index[word]\n",
        "    rank = 1-rank\n",
        "    # print(rank)\n",
        "    return rank\n",
        "\n"
      ],
      "metadata": {
        "id": "hCfvwg8JJF4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The index we defined"
      ],
      "metadata": {
        "id": "m3mJsgmSRyZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# indexService.load_from_db()\n",
        "# print(indexService.index_toString())"
      ],
      "metadata": {
        "id": "x7XHwAdzDX5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "firebaseService = FirebaseService()\n",
        "indexService = IndexService(firebaseService=firebaseService)\n",
        "indexService.load_from_db()\n",
        "crawlerService = CrawlerService(indexService, maxDepth=10)\n",
        "queryService = QueryService(indexService)\n",
        "query1 = queryService.query(\"PAAS\")\n",
        "query2 = queryService.query(\"SAAS OR PAAS\")\n",
        "print(query1)\n",
        "print(query2)\n",
        "print(queryService.get_history())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1EQzYldv35Jk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f9378365-ba63-42df-c0e9-e568a0c05a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfirebaseService = FirebaseService()\\nindexService = IndexService(firebaseService=firebaseService)\\nindexService.load_from_db()\\ncrawlerService = CrawlerService(indexService, maxDepth=10)\\nqueryService = QueryService(indexService)\\nquery1 = queryService.query(\"PAAS\")\\nquery2 = queryService.query(\"SAAS OR PAAS\")\\nprint(query1)\\nprint(query2)\\nprint(queryService.get_history())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphService:\n",
        "    def __init__(self, indexService, heatmap_output, bar_output):\n",
        "        \"\"\"\n",
        "        Initializes the GraphService with the reverse index data.\n",
        "        \"\"\"\n",
        "        self.indexService = indexService\n",
        "        self.index_df_coalition_heatmap = None\n",
        "        self.index_df_urls_heatmap = None\n",
        "        self.index_df_bar = None\n",
        "        self.heatmap_output, self.bar_output = heatmap_output, bar_output\n",
        "        self.buildDFs()\n",
        "\n",
        "    def buildDFs(self):\n",
        "        \"\"\"\n",
        "        Builds the dataframes for heatmap and bar chart from the reverse index data.\n",
        "        \"\"\"\n",
        "        rev_index = self.indexService.get_reverse_index()\n",
        "        self.index_df_coalition_heatmap = self.__get_shared_docs_dataframe(rev_index)\n",
        "        # Collect data for the heatmap\n",
        "        webpage_data = defaultdict(lambda: {\"words\": [], \"occurrences\": []})\n",
        "        for key, value in rev_index.items():\n",
        "            doc_ids = value.get(\"DocIDs\", [])\n",
        "            doc_ids_cntrs = value.get(\"DocIDs_cntrs\", [])\n",
        "            term = value.get(\"term\", \"\")\n",
        "\n",
        "            for doc_id, count in zip(doc_ids, doc_ids_cntrs):\n",
        "                webpage_data[doc_id][\"words\"].append(term)\n",
        "                webpage_data[doc_id][\"occurrences\"].append(count)\n",
        "\n",
        "        # Create DataFrame for heatmap\n",
        "        webpage_dataframes = {}\n",
        "        for page, content in webpage_data.items():\n",
        "            webpage_df = pd.DataFrame({\n",
        "                \"words\": content[\"words\"],\n",
        "                \"occurrences\": content[\"occurrences\"]\n",
        "            })\n",
        "            webpage_dataframes[page] = webpage_df\n",
        "\n",
        "        self.index_df_urls_heatmap = pd.DataFrame({\n",
        "            page: {row[\"words\"]: row[\"occurrences\"] for _, row in df.iterrows()}\n",
        "            for page, df in webpage_dataframes.items()\n",
        "        }).fillna(0)\n",
        "\n",
        "        # Create DataFrame for bar chart\n",
        "        word_totals = self.index_df_urls_heatmap.sum(axis=1).reset_index()\n",
        "        word_totals.columns = [\"word\", \"occurrences\"]\n",
        "        self.index_df_bar = word_totals\n",
        "\n",
        "        self.build_heatmap()\n",
        "        self.build_barChart()\n",
        "\n",
        "    def build_heatmap(self):\n",
        "      \"\"\"\n",
        "      Returns the heatmap graph wrapped in a widget output,\n",
        "      displaying only the lower triangle (without diagonal or upper triangle),\n",
        "      and keeping the original matrix size.\n",
        "      \"\"\"\n",
        "      with self.heatmap_output:\n",
        "          self.heatmap_output.clear_output()\n",
        "          print(\"Heatmap: Shared URLs between Indexes\")\n",
        "          print(\"The value at each cell [i,j] is the amount of shared urls between index i and index j\")\n",
        "\n",
        "          # Retrieve the matrix with only the lower triangle\n",
        "          df = self.index_df_coalition_heatmap\n",
        "\n",
        "          plt.figure(figsize=(13, 10))  # Increased size of the figure\n",
        "\n",
        "          # Display heatmap with lower triangle masked\n",
        "          sns.heatmap(\n",
        "              df,\n",
        "              annot=True,\n",
        "              cmap=\"YlGnBu\",\n",
        "              linewidths=0.5,\n",
        "              fmt=\"g\",\n",
        "              cbar_kws={'label': 'Occurrences'},\n",
        "              square=True,  # Ensure that it's square\n",
        "              annot_kws={'size': 10},  # Adjust annotation text size\n",
        "              mask=df.where(np.triu(np.ones(df.shape), k=1).astype(bool)),  # Mask upper triangle\n",
        "              cbar=True,  # Show colorbar\n",
        "          )\n",
        "\n",
        "          # Set title and labels for the plot\n",
        "          plt.title(\"Heatmap: Shared URLs between Indexes\", fontsize=14)\n",
        "          plt.xlabel(\"Index 1\", fontsize=12)\n",
        "          plt.ylabel(\"Index 2\", fontsize=12)\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.tight_layout()  # Ensure labels don't overlap\n",
        "\n",
        "          plt.show()\n",
        "\n",
        "      return self.heatmap_output\n",
        "\n",
        "\n",
        "    def build_barChart(self):\n",
        "        \"\"\"\n",
        "        Returns the bar chart graph wrapped in a widget output, with bars sorted in descending order.\n",
        "        \"\"\"\n",
        "        with self.bar_output:\n",
        "            self.bar_output.clear_output()\n",
        "            print(\"Bar Chart: Word Occurrences Across Webpages\")\n",
        "            print(\"The value of index i is the total amount of occurrences that index showed up in the pages\")\n",
        "\n",
        "            # Sort the DataFrame in descending order of occurrences\n",
        "            sorted_df = self.index_df_bar.sort_values(by=\"occurrences\", ascending=False)\n",
        "\n",
        "            plt.figure(figsize=(14, 8))\n",
        "            sns.barplot(data=sorted_df, x=\"word\", y=\"occurrences\", hue=\"word\", palette=\"viridis\")\n",
        "            plt.title(\"Bar Chart: Word Occurrences Across Webpages (Sorted)\", fontsize=16)\n",
        "            plt.xlabel(\"Words\", fontsize=14)\n",
        "            plt.ylabel(\"Occurrences\", fontsize=14)\n",
        "            plt.xticks(rotation=45, fontsize=10)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        return self.bar_output\n",
        "\n",
        "\n",
        "\n",
        "    def __get_shared_docs_dataframe(self, reverse_index):\n",
        "      \"\"\"\n",
        "      Creates a dataframe of shared documents between terms.\n",
        "      Only retains the lower triangle of the matrix, and sets the upper triangle (including the diagonal) to 0 or NaN.\n",
        "      \"\"\"\n",
        "\n",
        "      # Function to count shared documents between two lists of DocIDs\n",
        "      count_shared_docs = lambda docs1, docs2: len(set(docs1) & set(docs2))\n",
        "\n",
        "      # Getting the list of indices (terms)\n",
        "      indices = list(reverse_index.keys())\n",
        "      data = {}\n",
        "\n",
        "      # Loop through each pair of indices and compute shared document counts\n",
        "      for index1 in indices:\n",
        "          data[reverse_index[index1][\"term\"]] = {}\n",
        "          for index2 in indices:\n",
        "              shared_docs_count = count_shared_docs(\n",
        "                  reverse_index[index1][\"DocIDs\"], reverse_index[index2][\"DocIDs\"]\n",
        "              )\n",
        "              # Store values below the diagonal, set above diagonal and diagonal to 0\n",
        "              if indices.index(index1) < indices.index(index2):\n",
        "                  data[reverse_index[index1][\"term\"]][reverse_index[index2][\"term\"]] = shared_docs_count\n",
        "              else:\n",
        "                  data[reverse_index[index1][\"term\"]][reverse_index[index2][\"term\"]] = 0\n",
        "\n",
        "      # Convert the dictionary to DataFrame\n",
        "      df = pd.DataFrame(data)\n",
        "\n",
        "      # Mask the upper triangle (including diagonal) to be zero\n",
        "      mask = np.triu(np.ones_like(df, dtype=bool))  # Creates an upper triangle mask\n",
        "      df = df.mask(mask)  # Apply the mask to zero out the upper triangle\n",
        "\n",
        "      return df\n"
      ],
      "metadata": {
        "id": "PCUrCkXjyYuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SearchEngineUI:\n",
        "    def __init__(self, queryService, history_service):\n",
        "        \"\"\"\n",
        "        Initializes the SearchEngineUI with the queryService and history_service.\n",
        "        \"\"\"\n",
        "        desc = widgets.Output()\n",
        "        with desc:\n",
        "          print(\"To query please enter a text into the field and press the 'Search' button\")\n",
        "          print(\"Each page in the result is ranked by the amount the query's keywords appear in the page's text\")\n",
        "          print()\n",
        "        self.queryService = queryService\n",
        "        self.history_service = history_service  # Link to the history service\n",
        "        self.query_input = widgets.Text(\n",
        "            placeholder=\"Enter your search query here...\",\n",
        "            description=\"Query:\",\n",
        "            layout=widgets.Layout(width='70%')\n",
        "        )\n",
        "        self.search_button = widgets.Button(\n",
        "            description=\"Search\",\n",
        "            button_style=\"primary\",\n",
        "            tooltip=\"Click to search\",\n",
        "            icon=\"search\"\n",
        "        )\n",
        "        self.results_output = widgets.Output()\n",
        "        self.pagination_controls = widgets.HBox([])  # Placeholder for pagination controls\n",
        "        self.result_count_label = widgets.Label(value=\"\")  # Label for displaying result count\n",
        "\n",
        "        self.current_query = None\n",
        "        self.current_results = []\n",
        "        self.current_page = 0\n",
        "\n",
        "        # Attach event handlers\n",
        "        self.query_input.on_submit(self.perform_search)\n",
        "        self.search_button.on_click(self.perform_search)\n",
        "\n",
        "        # Layout the GUI\n",
        "        self.gui = widgets.VBox([\n",
        "            desc,\n",
        "            widgets.HBox([self.query_input, self.search_button]),\n",
        "            self.result_count_label,  # Add the result count label\n",
        "            self.results_output,\n",
        "            self.pagination_controls\n",
        "        ])\n",
        "\n",
        "    def display(self):\n",
        "        \"\"\"\n",
        "        Displays the GUI for the SearchEngineUI.\n",
        "        \"\"\"\n",
        "        display(self.gui)\n",
        "\n",
        "    def query(self, query_str):\n",
        "        \"\"\"Executes a query and returns the results.\"\"\"\n",
        "        try:\n",
        "            results = self.queryService.query(query_str)  # Assuming queryService.query() returns a list of JSON results\n",
        "            # Sort results by rank in descending order\n",
        "            sorted_results = sorted(results, key=lambda x: x['rank'], reverse=True)\n",
        "            return sorted_results\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the query: {e}\")\n",
        "            return []\n",
        "\n",
        "    def perform_search(self, b):\n",
        "        \"\"\"\n",
        "        Executes the search and displays the first page of results.\n",
        "        \"\"\"\n",
        "        query = self.query_input.value.strip()\n",
        "        if not query:\n",
        "            with self.results_output:\n",
        "                self.results_output.clear_output()\n",
        "                print(\"Please enter a search query.\")\n",
        "            return\n",
        "\n",
        "        startTime = time.time()\n",
        "        # Execute the query and save the results\n",
        "        self.current_query = query\n",
        "        self.current_results = self.query(query)\n",
        "        self.current_page = 0\n",
        "\n",
        "        # Save the query and results in history service\n",
        "        # if self.current_results:\n",
        "        self.history_service.save_search(query, self.current_results)\n",
        "\n",
        "        # Update the result count label\n",
        "        endTime = time.time()\n",
        "        self.update_result_count(len(self.current_results), endTime-startTime)\n",
        "\n",
        "        # Display the first page of results\n",
        "        self.display_page(self.current_page)\n",
        "\n",
        "    def update_result_count(self, count, time_took):\n",
        "        \"\"\"\n",
        "        Updates the label displaying the number of results received.\n",
        "        \"\"\"\n",
        "        self.result_count_label.value = f\"Number of results: {count} (took {time_took:.3f} sec)\"\n",
        "\n",
        "    def display_page(self, page):\n",
        "        \"\"\"\n",
        "        Displays a specific page of results.\n",
        "        \"\"\"\n",
        "        with self.results_output:\n",
        "            self.results_output.clear_output()\n",
        "            start = page * 10\n",
        "            end = start + 10\n",
        "            chunk = self.current_results[start:end]\n",
        "\n",
        "            if not chunk:\n",
        "                print(\"No results to display.\")\n",
        "                self.update_pagination_controls()\n",
        "                return\n",
        "\n",
        "            # Convert results to a DataFrame and display clickable links\n",
        "            results_df = pd.DataFrame(chunk)\n",
        "            if 'url' in results_df.columns:\n",
        "                results_df = results_df[['url']]\n",
        "                results_df['url'] = results_df['url'].apply(\n",
        "                    lambda x: f'<a href=\"{x}\" target=\"_blank\">{x}</a>'\n",
        "                )\n",
        "                numbered_results = []\n",
        "                for i, url in enumerate(results_df['url']):\n",
        "                  rank_of_url = (self.current_results[start+i]['rank'])*100\n",
        "                  if rank_of_url < 0.01:\n",
        "                    rank_of_url = 0.01\n",
        "                  numbered_results.append(\n",
        "                    f'<div style=\"background-color: {\"#f9f9f9\" if i % 2 == 0 else \"#eaeaea\"}; padding: 8px;\">'\n",
        "                    f\"{i + 1 + start}. {url} (match: {rank_of_url:.2f}%)<br>\"\n",
        "                    f\"<small style='color: gray;'>{(self.current_results[start+i]['snippet'])}...</small>\"\n",
        "                    f\"</div>\"\n",
        "                  )\n",
        "\n",
        "                html = '<div style=\"text-align: left; font-family: Arial; font-size: 14px;\">' + ''.join(\n",
        "                    numbered_results\n",
        "                ) + '</div>'\n",
        "                display(HTML(html))\n",
        "\n",
        "        # Update pagination controls\n",
        "        self.update_pagination_controls(page)\n",
        "\n",
        "    def update_pagination_controls(self, page=None):\n",
        "        \"\"\"\n",
        "        Updates the pagination controls based on the current page.\n",
        "        \"\"\"\n",
        "        if page == None:\n",
        "          self.pagination_controls.children = []\n",
        "          return\n",
        "\n",
        "        total_pages = (len(self.current_results)) // 10 + (len(self.current_results) % 10 !=0)  # Calculate total pages\n",
        "\n",
        "        # Create Previous and Next buttons\n",
        "        prev_button = widgets.Button(\n",
        "            description=\"Previous\",\n",
        "            icon=\"arrow-left\",\n",
        "            button_style=\"info\",\n",
        "            disabled=(page == 0)  # Disable if on the first page\n",
        "        )\n",
        "        next_button = widgets.Button(\n",
        "            description=\"Next\",\n",
        "            icon=\"arrow-right\",\n",
        "            button_style=\"info\",\n",
        "            disabled=(page == total_pages - 1)  # Disable if on the last page\n",
        "        )\n",
        "\n",
        "        # Attach event handlers\n",
        "        prev_button.on_click(lambda b: self.display_page(page - 1))\n",
        "        next_button.on_click(lambda b: self.display_page(page + 1))\n",
        "        prev_button.layout.display = (\"flex\" if page != 0 else \"none\")\n",
        "        next_button.layout.display = (\"flex\" if page != total_pages - 1 else \"none\")\n",
        "\n",
        "        label=widgets.Label(f\"Page {page + 1} of {total_pages}\")\n",
        "\n",
        "        # Update the pagination controls layout\n",
        "        self.pagination_controls.children = [prev_button, label, next_button] if total_pages else []\n"
      ],
      "metadata": {
        "id": "bq4xyvvUCcep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#משימה ששלומי נתן לישראל 31.12.24\n",
        "#בוצעה\n",
        "\n",
        "class SearchHistoryUI:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the SearchHistoryUI.\n",
        "        \"\"\"\n",
        "        self.history = []  # List to store search history\n",
        "        self.history_output = widgets.Output()  # Output widget for displaying history\n",
        "        with self.history_output:\n",
        "          print(\"You have no history.\")\n",
        "\n",
        "    def save_search(self, query, results):\n",
        "        \"\"\"\n",
        "        Saves the search query and its results to the history.\n",
        "        \"\"\"\n",
        "        self.history.append({\"query\": query, \"results\": results})\n",
        "        self.history = self.history[-5:]  # Keep only the last 5 searches\n",
        "        self.display_history()  # Update the history display\n",
        "\n",
        "    def display_history(self):\n",
        "        \"\"\"\n",
        "        Displays the 5 most recent search results in the History Service tab with numbering and result counts.\n",
        "        \"\"\"\n",
        "        with self.history_output:\n",
        "            self.history_output.clear_output()\n",
        "\n",
        "            if not self.history:\n",
        "                print(\"No recent searches available.\")\n",
        "            else:\n",
        "                accordion = widgets.Accordion()  # Create an accordion for collapsible lists\n",
        "                for i, entry in enumerate(reversed(self.history), start=1):  # Most recent first\n",
        "                    # Create HTML for the list of URLs\n",
        "                    results_html = \"<ul>\"\n",
        "                    for site_index, result in enumerate(entry[\"results\"][:10], start=1):\n",
        "                        results_html += f'<li>{site_index}. <a href=\"{result[\"url\"]}\" target=\"_blank\">{result[\"url\"]}</a></li>'\n",
        "                    results_html += \"</ul>\"\n",
        "\n",
        "                    # Create an output widget to hold the list of URLs\n",
        "                    result_output = widgets.Output()\n",
        "                    with result_output:\n",
        "                        display(HTML(results_html))\n",
        "\n",
        "                    # Add the query with result count as a button with collapsible functionality\n",
        "                    result_count = len(entry[\"results\"])\n",
        "                    accordion.children += (result_output,)\n",
        "                    # accordion.set_title(i - 1, f\"{i}. Query: {entry['query']} (Results: {result_count})\")\n",
        "                    accordion.set_title(i - 1, f\"Query No.{i}: {entry['query']} {'(no results)' if not len(entry['results']) else ''}\")\n",
        "\n",
        "                # Display the accordion\n",
        "                display(accordion)\n",
        "\n",
        "    def get_output_widget(self):\n",
        "        \"\"\"\n",
        "        Returns the output widget for the History Service tab.\n",
        "        \"\"\"\n",
        "        return self.history_output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zJ0XwBgn400b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ChatbotUI:\n",
        "\n",
        "  def __init__(self, indexService):\n",
        "    genai.configure(api_key='AIzaSyDURb6iohpm_goSIdOB9keMvRXnx88D9p8')\n",
        "    self.model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "    self.indexService = indexService\n",
        "    self.__initBot()\n",
        "    self.__buildGUI()\n",
        "    pass\n",
        "\n",
        "  def display(self):\n",
        "    display(self.gui)\n",
        "\n",
        "  def __initBot(self):\n",
        "    self.term_summaries = {\n",
        "        'SAAS': \"Software as a Service (SaaS) is a cloud computing model that delivers software applications over the internet, allowing users to access them without needing to install or maintain hardware or software, as everything is managed by the provider.\",\n",
        "        'PAAS': \"Platform as a Service (PaaS) is a cloud computing offering that provides developers with an environment to build, deploy, and manage applications without worrying about the underlying infrastructure, allowing for faster development.\",\n",
        "        'IAAS': \"Infrastructure as a Service (IaaS) is a cloud computing model that provides virtualized computing resources like servers, storage, and networking on demand, offering flexibility and scalability for businesses.\",\n",
        "        'FAAS': \"Function as a Service (FaaS) is a cloud computing model that enables developers to execute code in response to events without managing servers, offering a scalable and cost-efficient way to build applications.\",\n",
        "        'Private': \"Private cloud refers to cloud computing resources used exclusively by one organization, providing greater control, security, and customization compared to public cloud solutions.\",\n",
        "        'Public': \"Public cloud refers to cloud services offered over the public internet by third-party providers, accessible to anyone and known for scalability, cost efficiency, and ease of use.\",\n",
        "        'Hybrid': \"Hybrid cloud combines private and public cloud infrastructures, allowing data and applications to be shared between them, offering flexibility and optimization of existing infrastructure.\",\n",
        "        'Service': \"In computing, service refers to a functionality or resource provided to users or applications, often delivered via cloud computing or network systems.\",\n",
        "        'Platform': \"A platform is a foundation or environment that enables the development, deployment, and management of applications and services, often abstracting underlying infrastructure.\",\n",
        "        'Infrastructure': \"Infrastructure in computing refers to the hardware, software, networks, and facilities required to support the development, deployment, and operation of applications and IT systems.\",\n",
        "        'Study': \"A study refers to a detailed investigation or analysis of a subject or phenomenon, often conducted to gain deeper insights or inform decisions.\",\n",
        "        'Case': \"A case in this context often refers to a specific instance or example studied to understand a phenomenon, process, or system in detail.\",\n",
        "        'Chatbot': \"A chatbot is a software application designed to simulate human conversation, typically using artificial intelligence and natural language processing.\",\n",
        "        'Engine': \"An engine in computing typically refers to a core component or system that performs essential processing or computational tasks, such as a search engine or rendering engine.\",\n",
        "        'Cloud': \"Cloud computing refers to delivering computing services over the internet, including storage, processing, and software, allowing for scalable and on-demand resources.\",\n",
        "        'Monitor': \"Monitoring in IT refers to the continuous observation and analysis of systems, applications, or networks to ensure performance, reliability, and security.\",\n",
        "        'Data': \"Data refers to information, often in digital form, that can be processed, analyzed, and used to make decisions or derive insights.\",\n",
        "        'Mainframe': \"A mainframe is a powerful, high-performance computer used primarily by large organizations for critical applications, bulk data processing, and enterprise resource planning.\",\n",
        "        'Performance': \"Performance in IT refers to the effectiveness and efficiency of a system, application, or network in executing tasks or meeting user requirements.\",\n",
        "        'Security': \"Security in computing refers to measures and practices designed to protect systems, networks, and data from unauthorized access, attacks, or damage.\",\n",
        "        'SLA': \"A Service Level Agreement (SLA) is a formal contract between a service provider and a customer that defines the level of service expected, including performance metrics and responsibilities.\",\n",
        "        'KPI': \"Key Performance Indicators (KPIs) are measurable values that demonstrate how effectively an individual, team, or organization is achieving specific objectives.\",\n",
        "        'SOA': \"Service-Oriented Architecture (SOA) is a software design approach where services are provided to other components through a communication protocol, enabling flexibility and reusability.\",\n",
        "        'Information': \"Information refers to processed, organized, or structured data that is meaningful and useful for decision-making or understanding.\",\n",
        "        'Kafka': \"Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications, known for its high throughput and scalability.\",\n",
        "        'SQL': \"Structured Query Language (SQL) is a programming language used for managing and querying relational databases, enabling efficient data manipulation and retrieval.\",\n",
        "        'Technology': \"Technology refers to the application of scientific knowledge for practical purposes, especially in industry, computing, and innovation.\",\n",
        "        'Database': \"A database is an organized collection of data that can be easily accessed, managed, and updated, typically using database management systems.\",\n",
        "        'Docker': \"Docker is a platform for developing, shipping, and running applications in lightweight containers, ensuring consistency across development and production environments.\",\n",
        "        'Kubernetes': \"Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.\",\n",
        "        'RabbitMQ': \"RabbitMQ is an open-source message broker that facilitates communication between distributed systems by queuing and delivering messages reliably.\",\n",
        "        'IBM': \"IBM (International Business Machines Corporation) is a multinational technology company known for its innovations in computing, cloud solutions, and enterprise IT services.\",\n",
        "        'Google': \"Google is a global technology company specializing in internet-related services and products, including search engines, cloud computing, and AI advancements.\",\n",
        "        'Amazon': \"Amazon is a multinational technology company known for its e-commerce, cloud computing services (AWS), and advancements in artificial intelligence.\",\n",
        "        'AI': \"Artificial Intelligence (AI) is a field of computer science focused on creating systems capable of performing tasks that typically require human intelligence, such as learning, reasoning, and problem-solving.\",\n",
        "        'Artificial': \"Artificial refers to something created or simulated by humans, often to replicate natural phenomena or functionalities, such as artificial intelligence.\",\n",
        "        'Intelligence': \"Intelligence refers to the ability to acquire and apply knowledge and skills, often associated with problem-solving and decision-making capabilities in humans or machines.\"\n",
        "    }\n",
        "\n",
        "    self.term_summaries = {\n",
        "        k.lower(): v for k, v in self.term_summaries.items()\n",
        "    }\n",
        "\n",
        "    # reflections.update(self.term_summaries)\n",
        "\n",
        "    # for t,val in self.indexService.get_reverse_index().items():\n",
        "    #   term_summaries[t] = self.__queryGENAI(f\"explain '{val['term']}', tell me in one paragrath\")\n",
        "    patterns = [\n",
        "        (r\"^what is (\\w+)$\", [\"%1\"]),\n",
        "        (r\"^explain (\\w+)$\", [\"%1\"]),\n",
        "        (r\"^(\\w+)$\", [\"%1\"]),\n",
        "        (r\".*\", [\"Sorry, I did not understand that. I only know terms.\"]),\n",
        "    ]\n",
        "\n",
        "    self.chatbot = Chat(patterns, self.term_summaries)\n",
        "\n",
        "  # def __queryGENAI(self, query):\n",
        "  #   response = self.model.generate_content(query)\n",
        "  #   return self.__to_markdown(response.text)\n",
        "\n",
        "  def __buildGUI(self):\n",
        "\n",
        "        desc = widgets.Output()\n",
        "        with desc:\n",
        "          print(\"To talk with the chatbot enter text into the field and press the 'Send' button\")\n",
        "          print()\n",
        "        self.msg_input = widgets.Text(\n",
        "            placeholder=\"Enter your message here (I only know terms)\",\n",
        "            description=\"Message:\",\n",
        "            layout=widgets.Layout(width='70%')\n",
        "        )\n",
        "        self.send_button = widgets.Button(\n",
        "            description=\"Send\",\n",
        "            button_style=\"primary\",\n",
        "            tooltip=\"Click to search\",\n",
        "            icon=\"send\"\n",
        "        )\n",
        "        self.results_output = widgets.Output()\n",
        "\n",
        "        # Attach event handler\n",
        "        self.msg_input.on_submit(self.__perform_send)\n",
        "        self.send_button.on_click(self.__perform_send)\n",
        "\n",
        "        # Layout the GUI\n",
        "        self.gui = widgets.VBox([\n",
        "            desc,\n",
        "            widgets.HBox([self.msg_input, self.send_button]),\n",
        "            self.results_output\n",
        "        ])\n",
        "\n",
        "  def __perform_send(self, q):\n",
        "        msg = self.msg_input.value.strip().lower()\n",
        "        if not msg:\n",
        "            with self.results_output:\n",
        "                self.results_output.clear_output()\n",
        "                print(\"Please enter a message.\")\n",
        "            return\n",
        "\n",
        "        # Display results\n",
        "        with self.results_output:\n",
        "            self.results_output.clear_output()\n",
        "            print(\"Thinking...\")\n",
        "            # Get the results for the message\n",
        "            results = self.chatbot.respond(msg) if self.__termInMessage(msg) else \"Unknown term\"\n",
        "            self.results_output.clear_output()\n",
        "            if not results:\n",
        "                print(\"No results found for your term.\")\n",
        "            else:\n",
        "                # Assuming the response is a list of strings\n",
        "                if isinstance(results, list):\n",
        "                    response_text = \"\\n\".join(results)\n",
        "                else:\n",
        "                    response_text = str(results)  # Convert to string if not already\n",
        "                display(widgets.HTML(f\"<b>Response:</b> {response_text}\"))\n",
        "\n",
        "  def  __to_markdown(self, text):\n",
        "    text = text.replace('•', ' *')\n",
        "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "  def __termInMessage(self, msg):\n",
        "    words = re.findall(r'\\b\\w+\\b', msg)  # Find individual words\n",
        "    for word in words:\n",
        "        if word.lower() in self.term_summaries:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "Te4MfWpa36-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EditIndexUI:\n",
        "\n",
        "  def __init__(self, indexService, crawlerService, graphService):\n",
        "    self.indexService = indexService\n",
        "    self.crawlerService = crawlerService\n",
        "    self.opFunc = {\n",
        "        \"1\": self.__printAction,\n",
        "        \"2\": self.__addWordAction,\n",
        "        \"3\": self.__removeWordAction,\n",
        "        \"4\": self.__addURLAction,\n",
        "        \"5\": self.__removeURLAction,\n",
        "        \"6\": self.__saveAction,\n",
        "        \"7\": self.__loadAction,\n",
        "        \"8\": self.__crawlAction,\n",
        "        # \"9\": self.__exitAction,\n",
        "    }\n",
        "    self.opInputPrompt = {\n",
        "        \"2\": \"Please enter the word to be added: \",\n",
        "        \"3\": \"Please enter a word to be removed: \",\n",
        "        \"4\": \"Please enter a url to be added: \",\n",
        "        \"5\": \"Please enter a url to be removed: \",\n",
        "    }\n",
        "    self.selectFunc = None\n",
        "    self.graphService = graphService\n",
        "\n",
        "  def display(self):\n",
        "      self.menu_output = widgets.Output()\n",
        "      with self.menu_output:\n",
        "          print(\"\\nMenu:\")\n",
        "          print(\"\\t1. Print index\")\n",
        "          print(\"\\t2. Add a new word to index\")\n",
        "          print(\"\\t3. Remove word from index\")\n",
        "          print(\"\\t4. Add a new url to crawl\")\n",
        "          print(\"\\t5. Remove a url\")\n",
        "          print(\"\\t6. Save to FireBase\")\n",
        "          print(\"\\t7. Load from FireBase\")\n",
        "          print(\"\\t8. Start crawling\")\n",
        "          # op=input(\"Choose an option [1-8]: \")\n",
        "\n",
        "      self.msg_input = widgets.Text(\n",
        "          placeholder=\"[1-8]\",\n",
        "          description=\"Choose an option [1-8]: :\",\n",
        "          style={'description_width': \"auto\"},\n",
        "          layout=widgets.Layout(width='20%')\n",
        "      )\n",
        "      self.send_button = widgets.Button(\n",
        "          description=\"Send\",\n",
        "          button_style=\"primary\",\n",
        "          tooltip=\"Click to search\",\n",
        "          style={'description_width': \"auto\"},\n",
        "          icon=\"send\"\n",
        "      )\n",
        "\n",
        "      self.data_input = widgets.Text(\n",
        "          placeholder=\"Enter input\",\n",
        "          style={'description_width': \"auto\"},\n",
        "          layout=widgets.Layout(width='30%', display='none')\n",
        "      )\n",
        "      self.data_button = widgets.Button(\n",
        "          description=\"Submit\",\n",
        "          button_style=\"primary\",\n",
        "          tooltip=\"Click to search\",\n",
        "          style={'description_width': \"auto\"},\n",
        "          layout=widgets.Layout(display='none'),\n",
        "          icon=\"send\"\n",
        "      )\n",
        "\n",
        "      self.results_output = widgets.Output()\n",
        "\n",
        "      # Attach event handler\n",
        "      self.msg_input.on_submit(self.__perform_action)\n",
        "      self.send_button.on_click(self.__perform_action)\n",
        "      self.data_input.on_submit(self.__doActionOnClick)\n",
        "      self.data_button.on_click(self.__doActionOnClick)\n",
        "\n",
        "      self.gui = widgets.VBox([\n",
        "          self.menu_output,\n",
        "          widgets.HBox([self.msg_input, self.send_button]),\n",
        "          widgets.HBox([self.data_input, self.data_button]),\n",
        "          self.results_output\n",
        "      ])\n",
        "      display(self.gui)\n",
        "\n",
        "  def __perform_action(self, o):\n",
        "\n",
        "    self.op = self.msg_input.value.strip()\n",
        "    self.results_output.clear_output()\n",
        "    with self.results_output:\n",
        "\n",
        "        self.selectFunc = self.opFunc.get(self.op,None)\n",
        "        if self.selectFunc == None:\n",
        "          print(\"Invalid action\")\n",
        "        else:\n",
        "          self.data_input.value=\"\"\n",
        "          if (2<= int(self.op) <= 5):\n",
        "            self.data_input.description = self.opInputPrompt.get(self.op, \"ERROR\")\n",
        "            self.data_input.value=\"\"\n",
        "            self.data_input.layout.display = \"flex\"\n",
        "            self.data_button.layout.display = \"flex\"\n",
        "          else:\n",
        "            self.data_input.layout.display = \"none\"\n",
        "            self.data_button.layout.display = \"none\"\n",
        "            self.__toggleDisabled()\n",
        "            self.selectFunc()\n",
        "            self.__toggleDisabled()\n",
        "\n",
        "  def __toggleDisabled(self):\n",
        "    flag = not self.msg_input.disabled\n",
        "    self.msg_input.disabled = flag\n",
        "    self.send_button.disabled = flag\n",
        "    self.data_input.disabled = flag\n",
        "    self.data_button.disabled = flag\n",
        "\n",
        "\n",
        "  def __doActionOnClick(self, q):\n",
        "    with self.results_output:\n",
        "      if self.selectFunc:\n",
        "        self.__toggleDisabled()\n",
        "        self.selectFunc()\n",
        "        self.__toggleDisabled()\n",
        "      else:\n",
        "        print(\"Invalid action\")\n",
        "    self.data_input.layout.display = \"none\"\n",
        "    self.data_button.layout.display = \"none\"\n",
        "\n",
        "  def __printAction(self):\n",
        "    print(self.indexService.index_toString())\n",
        "\n",
        "  def __addWordAction(self):\n",
        "    w = self.data_input.value.strip()\n",
        "\n",
        "    if self.indexService.add_new_word(w):\n",
        "      print(f\"New word added '{w}': {self.indexService.get_index_of_word(w)}\")\n",
        "      print(\"A crawl is needed to build its index\")\n",
        "      self.graphService.buildDFs()\n",
        "      print(\"The graphs got updated\")\n",
        "    else:\n",
        "      print(\"The word was already in index\")\n",
        "\n",
        "  def __removeWordAction(self):\n",
        "    w = self.data_input.value.strip()\n",
        "\n",
        "    if self.indexService.remove_word(w):\n",
        "      print(f\"The word '{w}' was removed from index\")\n",
        "      self.graphService.buildDFs()\n",
        "      print(\"The graphs got updated\")\n",
        "    else:\n",
        "      print(f\"The word '{w}' wasn't in the index\")\n",
        "\n",
        "  def __addURLAction(self):\n",
        "    u = self.data_input.value.strip()\n",
        "    with self.results_output:\n",
        "      if not u: #Check if empty\n",
        "        print(\"Empty url\")\n",
        "      else:\n",
        "        msg = self.crawlerService.crawl_single_url(u)\n",
        "        print(\"Crawling result:\", msg)\n",
        "        self.graphService.buildDFs()\n",
        "        print(\"The graphs got updated\")\n",
        "\n",
        "  def __removeURLAction(self):\n",
        "    u = self.data_input.value.strip()\n",
        "\n",
        "    with self.results_output:\n",
        "      if self.indexService.remove_url(u):\n",
        "        print(\"Url removed from index\")\n",
        "        self.graphService.buildDFs()\n",
        "        print(\"The graphs got updated\")\n",
        "      else:\n",
        "        print(\"Url was not in index\")\n",
        "\n",
        "\n",
        "  def __saveAction(self):\n",
        "    self.indexService.save_in_db()\n",
        "\n",
        "  def __loadAction(self):\n",
        "    self.indexService.load_from_db()\n",
        "    print(\"index loaded from db\")\n",
        "    self.graphService.buildDFs() # was ..df(self.indexService.get_reverse_index())\n",
        "    print(\"The graphs got updated\")\n",
        "\n",
        "  def __crawlAction(self):\n",
        "    # TODO add try catch for int convert, and send to initCrawling\n",
        "    num = self.data_input.value.strip()\n",
        "    self.crawlerService.initCrawlingProcess()\n",
        "    self.graphService.buildDFs() # was ..df(self.indexService.get_reverse_index())\n",
        "    print(\"The graphs got updated\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0iaRyH2hUFpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#משימה ששלומי נתן לישראל 31.12.24\n",
        "#בוצעה\n",
        "\n",
        "def display_tabs(search_ui, history_service, graphService, chatbot_ui, editIndexUI):\n",
        "    \"\"\"\n",
        "    Creates and displays the tabs containing the search engine, history, and other features.\n",
        "    \"\"\"\n",
        "    # Create a container for the search engine UI in the first tab\n",
        "    query_service_output = widgets.Output()\n",
        "    with query_service_output:\n",
        "        search_ui.display()\n",
        "\n",
        "    # Get the output widget for the history service\n",
        "    history_service_output = history_service.get_output_widget()\n",
        "\n",
        "    # Create a container for the Chatbot UI\n",
        "    chatbot_service_output = widgets.Output()\n",
        "    with chatbot_service_output:\n",
        "        chatbot_ui.display()\n",
        "\n",
        "    # Create a container for the Edit Index Menu\n",
        "    index_menu_output = widgets.Output()\n",
        "    with index_menu_output:\n",
        "        editIndexUI.display()\n",
        "\n",
        "    # Create the tabs\n",
        "    tabs_toDisplay = [\n",
        "        {\"Search Engine\": query_service_output},\n",
        "        {\"Chatbot\": chatbot_service_output},\n",
        "        {\"History\": history_service_output},\n",
        "        {\"Index Coalitions\": graphService.heatmap_output},\n",
        "        {\"Index Total Occurrences\": graphService.bar_output},\n",
        "        {\"Index Menu\": index_menu_output},\n",
        "    ]\n",
        "    tabs = widgets.Tab(children=tuple(value for tab in tabs_toDisplay for value in tab.values()))\n",
        "    for i,k in enumerate([key for tab in tabs_toDisplay for key in tab.keys()]):\n",
        "      tabs.set_title(i, k)\n",
        "    display(tabs)\n"
      ],
      "metadata": {
        "id": "zi8ZwdK4yjWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initGUIProcess(indexService, editIndexUI, graphService):\n",
        "  queryService = QueryService(indexService)\n",
        "  # Assuming GraphService and indexService are already defined\n",
        "\n",
        "  # Define the history service output\n",
        "  history_output = SearchHistoryUI()\n",
        "\n",
        "  # Assuming SearchEngineUI and queryService are already defined\n",
        "  search_ui = SearchEngineUI(queryService,history_output)\n",
        "  chatbot_ui = ChatbotUI(indexService)\n",
        "\n",
        "  # SVG URL\n",
        "  phoenix_svg_url = \"https://raw.githubusercontent.com/ShlomiFridman/PhoenixProject2025/42847053ee0f661c5f25bc0d06ea7daf740e3cde/Project/phoenix-svgrepo-com.svg\"\n",
        "  ibm_svg_url = \"https://raw.githubusercontent.com/ShlomiFridman/PhoenixProject2025/e11c8ccf8c7ffe08da02f4df74393f6472f5bf51/Project/IBM_logo.svg\"\n",
        "\n",
        "  # Embed SVG with resizing\n",
        "  svg_resized_html = f'''\n",
        "  <div style=\"display: flex; align-items: center;\">\n",
        "    <div style=\"width: 100px; height: 100px;\">\n",
        "        <img src=\"{ibm_svg_url}\" style=\"width: 100%; height: 100%;\" />\n",
        "    </div>\n",
        "    &emsp;\n",
        "    <div style=\"width: 100px; height: 100px;\">\n",
        "        <img src=\"{phoenix_svg_url}\" style=\"width: 100%; height: 100%;\" />\n",
        "    </div>\n",
        "    <div style=\"margin-left: 10px; font-size: 20px; font-weight: bold;\">\n",
        "        Phoenix 2025 Search Engine\n",
        "    </div>\n",
        "  </div>\n",
        "  '''\n",
        "  # Display the SVG\n",
        "  display(HTML(svg_resized_html))\n",
        "\n",
        "  # Display the tabs with the search engine and other services\n",
        "  display_tabs(search_ui, history_output, graphService, chatbot_ui, editIndexUI)"
      ],
      "metadata": {
        "id": "tUS40GbXa6Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mainProcess():\n",
        "  firebaseService = FirebaseService()\n",
        "  indexService = IndexService(firebaseService=firebaseService)\n",
        "  indexService.load_from_db()\n",
        "  crawlerService = CrawlerService(indexService, maxDepth=10)\n",
        "  graphService = GraphService(indexService, widgets.Output(), widgets.Output())\n",
        "  editIndexUI = EditIndexUI(indexService, crawlerService, graphService)\n",
        "  initGUIProcess(indexService, editIndexUI, graphService)\n"
      ],
      "metadata": {
        "id": "VJsUQYR3aQF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mainProcess()"
      ],
      "metadata": {
        "id": "Lf0O-TJwbI3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improve UI"
      ],
      "metadata": {
        "id": "vIaNIS3O6ZgM"
      }
    }
  ]
}